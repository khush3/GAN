{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloaders\n",
    "from random import randint as ri\n",
    "import torch as torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('zero', 'one', 'two', 'three',\n",
    "           'four', 'five', 'six', 'seven', 'eight', 'nine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       two        two       three       four\n"
     ]
    }
   ],
   "source": [
    "#check if data is loaded properly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('      %4s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator model \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 32, out_channels = 16, kernel_size = (5), stride= 4),      #Apply a convolutional block\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 1, kernel_size = (14), stride= 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block(x);\n",
    "        \n",
    "        return abs(x%1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator()\n",
    "gen_ip = torch.randn(4,32,10,10)\n",
    "gen_ip.shape\n",
    "a = gen(gen_ip)\n",
    "imshow(torchvision.utils.make_grid(a[0]))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator model \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear( 24*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)         \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x);\n",
    "        x = x.view(-1,  24*4)\n",
    "        x = F.relu(self.fc1(x));\n",
    "        x = F.relu(self.fc2(x));\n",
    "        x = self.fc3(x);\n",
    "        #_, predicted = torch.max(x,1);\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the generator and discriminator\n",
    "gen = Generator()\n",
    "dis = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimization function and loss function for both generator and discriminator\n",
    "import torch.optim as optim\n",
    "\n",
    "criteriondis = nn.CrossEntropyLoss()\n",
    "criteriongen = nn.MSELoss()\n",
    "gen_optimizer = optim.Adam(gen.parameters(), lr=0.002, betas = (0.5,0.999))\n",
    "dis_optimizer = optim.Adam(dis.parameters(), lr = 0.00001, betas = (0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously trained weights:\n",
    "numpygen_conv1_weight = np.load('parameters/numpygen_conv1_weight.npy')\n",
    "numpygen_conv1_bias   = np.load('parameters/numpygen_conv1_bias.npy')\n",
    "numpygen_bn_weight    = np.load('parameters/numpygen_bn_weight.npy')\n",
    "numpygen_bn_bias      = np.load('parameters/numpygen_bn_bias.npy')\n",
    "numpygen_conv2_weight = np.load('parameters/numpygen_conv2_weight.npy')\n",
    "numpygen_conv2_bias   = np.load('parameters/numpygen_conv2_bias.npy')\n",
    "numpydis_conv1_weight = np.load('parameters/numpydis_conv1_weight.npy')\n",
    "numpydis_conv1_bias   = np.load('parameters/numpydis_conv1_bias.npy')\n",
    "numpydis_bn_weight    = np.load('parameters/numpydis_bn_weight.npy')\n",
    "numpydis_bn_bias      = np.load('parameters/numpydis_bn_bias.npy')\n",
    "\n",
    "#Write to the models\n",
    "#gen_conv1_weight = gen.block[0].weight\n",
    "#gen_conv1_bias = gen.block[0].bias\n",
    "#gen_bn_weight = gen.block[2].weight\n",
    "#gen_bn_bias = gen.block[2].bias\n",
    "#gen_conv2_weight = gen.block[3].weight\n",
    "#gen_conv2_bias = gen.block[3].bias\n",
    "#numpygen_conv1_weight = gen_conv1_weight.data.numpy()\n",
    "#numpygen_conv1_bias = gen_conv1_bias.data.numpy()\n",
    "#numpygen_bn_weight = gen_bn_weight.data.numpy()\n",
    "#numpygen_bn_bias = gen_bn_bias.data.numpy()\n",
    "#numpygen_conv2_weight = gen_conv2_weight.data.numpy()\n",
    "#numpygen_conv2_bias = gen_conv2_bias.data.numpy()\n",
    "#discriminator\n",
    "#dis_conv1_weight = dis.block[0].weight\n",
    "#dis_conv1_bias = dis.block[0].bias\n",
    "#dis_bn_weight = dis.block[3].weight\n",
    "#dis_bn_bias = dis.block[3].bias\n",
    "#numpydis_conv1_weight = dis_conv1_weight.data.numpy()\n",
    "#numpydis_conv1_bias = dis_conv1_bias.data.numpy()\n",
    "#numpydis_bn_weight = dis_bn_weight.data.numpy()\n",
    "#numpydis_bn_bias = dis_bn_bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Epoch: 1 | No of images:  2000 | real_loss: 2.328 | fake_loss: 0.000 | gen_loss: 1.994\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Epoch: 1 | No of images:  4000 | real_loss: 0.071 | fake_loss: 0.000 | gen_loss: 1.970\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Epoch: 1 | No of images:  6000 | real_loss: 0.001 | fake_loss: 0.000 | gen_loss: 1.935\n"
     ]
    }
   ],
   "source": [
    "#Train the GAN\n",
    "for epoch in range(1):\n",
    "\n",
    "    running_loss_real = 0.0\n",
    "    running_loss_fake = 0.0\n",
    "    running_loss_gen  = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        real_images, real_labels = data\n",
    "        ones = torch.ones(real_labels.shape,dtype = torch.int64)\n",
    "        zeros = torch.zeros(real_labels.shape,dtype = torch.int64)\n",
    "        gen_ip = torch.randn((4,32,10,10))\n",
    "\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        gen_op = gen(gen_ip)\n",
    "        dis_op = dis(real_images)\n",
    "        \n",
    "        #Minimize discriminator loss against ones\n",
    "        real_output = dis(real_images)\n",
    "        real_loss = criteriondis(real_output.float(), ones)\n",
    "        real_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        #Minimize discriminator loss against zeros\n",
    "        fake_images = gen(gen_ip)\n",
    "        fake_output = dis(fake_images)\n",
    "        fake_loss = criteriondis(fake_output.float(), zeros)\n",
    "        fake_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        #Minimize generator loss\n",
    "        fake_images = gen(gen_ip)\n",
    "        gen_loss = criteriongen(fake_images.float(), real_images.float())\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_real += real_loss.item()\n",
    "        running_loss_real += fake_loss.item()\n",
    "        running_loss_gen += gen_loss.item()\n",
    "        if i%200 == 199:\n",
    "            print('.')\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('Epoch: %d | No of images: %5d | real_loss: %.3f | fake_loss: %.3f | gen_loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss_real / 2000, running_loss_fake / 2000, running_loss_gen / 2000))\n",
    "            running_loss_real = 0.0\n",
    "            running_loss_fake = 0.0\n",
    "            running_loss_gen  = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = gen(gen_ip)\n",
    "imshow(torchvision.utils.make_grid(temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the learned weights and biases\n",
    "\n",
    "#generator\n",
    "gen_conv1_weight = gen.block[0].weight\n",
    "gen_conv1_bias = gen.block[0].bias\n",
    "gen_bn_weight = gen.block[2].weight\n",
    "gen_bn_bias = gen.block[2].bias\n",
    "gen_conv2_weight = gen.block[3].weight\n",
    "gen_conv2_bias = gen.block[3].bias\n",
    "numpygen_conv1_weight = gen_conv1_weight.data.numpy()\n",
    "numpygen_conv1_bias = gen_conv1_bias.data.numpy()\n",
    "numpygen_bn_weight = gen_bn_weight.data.numpy()\n",
    "numpygen_bn_bias = gen_bn_bias.data.numpy()\n",
    "numpygen_conv2_weight = gen_conv2_weight.data.numpy()\n",
    "numpygen_conv2_bias = gen_conv2_bias.data.numpy()\n",
    "#discriminator\n",
    "dis_conv1_weight = dis.block[0].weight\n",
    "dis_conv1_bias = dis.block[0].bias\n",
    "dis_bn_weight = dis.block[3].weight\n",
    "dis_bn_bias = dis.block[3].bias\n",
    "numpydis_conv1_weight = dis_conv1_weight.data.numpy()\n",
    "numpydis_conv1_bias = dis_conv1_bias.data.numpy()\n",
    "numpydis_bn_weight = dis_bn_weight.data.numpy()\n",
    "numpydis_bn_bias = dis_bn_bias.data.numpy()\n",
    "\n",
    "\n",
    "#save the parameters\n",
    "np.save('parameters/numpygen_conv1_weight', numpygen_conv1_weight)\n",
    "np.save('parameters/numpygen_conv1_bias', numpygen_conv1_bias)\n",
    "np.save('parameters/numpygen_bn_weight', numpygen_bn_weight)\n",
    "np.save('parameters/numpygen_bn_bias', numpygen_bn_bias)\n",
    "np.save('parameters/numpygen_conv2_weight', numpygen_conv2_weight)\n",
    "np.save('parameters/numpygen_conv2_bias', numpygen_conv2_bias)\n",
    "np.save('parameters/numpydis_conv1_weight', numpydis_conv1_weight)\n",
    "np.save('parameters/numpydis_conv1_bias', numpydis_conv1_bias)\n",
    "np.save('parameters/numpydis_bn_weight', numpydis_bn_weight)\n",
    "np.save('parameters/numpydis_bn_bias', numpydis_bn_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gen.state_dict(), 'parameters/gen')\n",
    "torch.save(dis.state_dict(), 'parameters/dis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
